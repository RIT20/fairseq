{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convin.Ai",
      "provenance": [],
      "mount_file_id": "1Ye-b5uRVovW7RNLVwctU_P60aIvhUMrD",
      "authorship_tag": "ABX9TyNt9e6Q6J33l3EuebkVvzqA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "58ac28e0673841efa9306a2275899398": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a450a9bead0b4d6499ac7ef45ece1321",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e29354cdf8f7491db63d10cd3f71356a",
              "IPY_MODEL_c297f04ce69a436ba88a84deeaa46ad5",
              "IPY_MODEL_ef64c5cb3bbf4300b4cab4f410a291f2"
            ]
          }
        },
        "a450a9bead0b4d6499ac7ef45ece1321": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e29354cdf8f7491db63d10cd3f71356a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a21e2d662ae1460bb1fd29af3f757065",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e0353b98cd7e43679e2b8973c14ad6ba"
          }
        },
        "c297f04ce69a436ba88a84deeaa46ad5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3f988e1b966942c88290f8e75526bfe1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 159,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 159,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2902659fdc9e496f8c7fd9c24505db52"
          }
        },
        "ef64c5cb3bbf4300b4cab4f410a291f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_51cbcdbc22d548c082c7a3b05b70d7b2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 159/159 [00:00&lt;00:00, 2.39kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3ac746633fdf450d8130a5469b4ce9cc"
          }
        },
        "a21e2d662ae1460bb1fd29af3f757065": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e0353b98cd7e43679e2b8973c14ad6ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3f988e1b966942c88290f8e75526bfe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2902659fdc9e496f8c7fd9c24505db52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "51cbcdbc22d548c082c7a3b05b70d7b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3ac746633fdf450d8130a5469b4ce9cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RIT20/fairseq/blob/master/Convin_Ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IirifuDcf21f",
        "outputId": "d779c36c-2a70-4677-872a-885a4cc76b1e"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 42.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 50.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 46.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.1\n",
            "Collecting datasets\n",
            "  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\n",
            "\u001b[K     |████████████████████████████████| 264 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting fsspec>=2021.05.0\n",
            "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 54.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 44.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Collecting tqdm>=4.42\n",
            "  Downloading tqdm-4.62.0-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.6.1)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.12)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: tqdm, xxhash, fsspec, datasets\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed datasets-1.11.0 fsspec-2021.7.0 tqdm-4.62.0 xxhash-2.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ty6W-HrLayf"
      },
      "source": [
        "pip install pydub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1-_ah04gB7X"
      },
      "source": [
        "import soundfile as sf\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "from IPython.display import Audio\n",
        "from scipy.io import wavfile\n",
        "import numpy as np\n",
        "import librosa, librosa.display"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "58ac28e0673841efa9306a2275899398",
            "a450a9bead0b4d6499ac7ef45ece1321",
            "e29354cdf8f7491db63d10cd3f71356a",
            "c297f04ce69a436ba88a84deeaa46ad5",
            "ef64c5cb3bbf4300b4cab4f410a291f2",
            "a21e2d662ae1460bb1fd29af3f757065",
            "e0353b98cd7e43679e2b8973c14ad6ba",
            "3f988e1b966942c88290f8e75526bfe1",
            "2902659fdc9e496f8c7fd9c24505db52",
            "51cbcdbc22d548c082c7a3b05b70d7b2",
            "3ac746633fdf450d8130a5469b4ce9cc"
          ]
        },
        "id": "QdburYbkgD-z",
        "outputId": "ca8e30ab-0676-498a-e58a-08da826a2f0e"
      },
      "source": [
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58ac28e0673841efa9306a2275899398",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/159 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5goOtBx2hsK-",
        "outputId": "37ad6ece-5173-4fd4-cc91-d593ba8ccf10"
      },
      "source": [
        "from pydub import AudioSegment\n",
        "sound = AudioSegment.from_mp3(\"/content/drive/MyDrive/Convin.Ai/sample_audio_1.mp3\")\n",
        "sound.export(\"/content/drive/MyDrive/Convin.Ai/sample_audio_1.wav\", format=\"wav\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_io.BufferedRandom name='/content/drive/MyDrive/Convin.Ai/sample_audio_1.wav'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0V_fY7GjuhX"
      },
      "source": [
        "file_name = '/content/drive/MyDrive/Convin.Ai/sample_audio_1.wav'\n",
        "Audio(file_name)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDoYJgyMju5_"
      },
      "source": [
        "data = wavfile.read(file_name)\n",
        "framerate = data[0]\n",
        "sounddata = data[1]\n",
        "time = np.arange(0,len(sounddata))/framerate\n",
        "print('Sample rate:',framerate,'Hz')\n",
        "print('Total time:',len(sounddata)/framerate,'s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht2lswWJkrLM"
      },
      "source": [
        "input_audio, sample_rate = librosa.load(file_name, \n",
        "                              sr=16000)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "sJSqAOvYmjXs",
        "outputId": "ae8e5783-3713-405a-ab12-18e4afc2a1d3"
      },
      "source": [
        "\n",
        "#plot between amplitude and time\n",
        "librosa.display.waveplot(input_audio, sr = sample_rate)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PolyCollection at 0x7fd52716d590>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3GAbJ4Ngk2c"
      },
      "source": [
        "input_values = processor(input_audio, sampling_rate=sample_rate, return_tensors=\"pt\").input_values"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmcdmET7gp7U"
      },
      "source": [
        "logits = model(input_values).logits\n",
        "predicted_ids = torch.argmax(logits, dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj53dfWJgtOz"
      },
      "source": [
        "transcription = processor.decode(predicted_ids[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSB0OYYHk7wE"
      },
      "source": [
        "input_values = tokenizer(input_audio, return_tensors=\"pt\").input_values\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6etC0zsUnrbU"
      },
      "source": [
        "logits = model(input_values).logits\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg9lCI8UoQpq"
      },
      "source": [
        "predicted_ids = torch.argmax(logits, dim=-1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NBHw2LalPPC"
      },
      "source": [
        "transcription = tokenizer.decode(predicted_ids[0])\n",
        "#generated transcript\n",
        "transcription"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Fkg-EBXgxHp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "ba88c946-aed5-48d6-d266-76d22c313b7f"
      },
      "source": [
        "\n",
        "target_transcription = open(\"/content/drive/MyDrive/Convin.Ai/task_Script.txt\", \"r\")\n",
        "target_transcription.read()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"HELLO EVERYONE THIS IS JANEED HERE FROM\\nEDURAKA AND I WELCOME YOU ALL TO THIS\\nINTERESTING SESSION\\nWHERE WE ARE GOING TO TURN ANY IMAGE\\nTHAT WE GIVE INTO A CARTOONS\\nSO WITHOUT ANY FURTHER ADO LET ME\\nQUICKLY WALK YOU THROUGH TODAY'S AGENDA\\nWE'LL START THIS SESSION BY\\nUNDERSTANDING THE PROBLEM STATEMENT AND\\nWHAT EXACTLY WOULD BE ITS WORKFLOW\\nFOLLOWING THAT I'LL BE WALKING YOU\\nTHROUGH THE DIFFERENT TOOLS AND\\nFRAMEWORKS THAT WE'RE GOING TO USE\\nAND FINALLY WE'LL START BUILDING UP THIS\\nPROJECT ALL RIGHT SO BEFORE WE BEGIN DO\\nCONSIDER SUBSCRIBING TO OUR YOUTUBE\\nCHANNEL AND HIT THE BELL ICON TO STAY\\nUPDATED ON TRENDING TECHNOLOGIES\\nAND ALSO IF YOU'RE LOOKING FORWARD FOR\\nPYTHON TRAINING CERTIFICATION PLEASE\\nCHECK OUT THE LINK GIVEN IN THE\\nDESCRIPTION BOX BELOW\\nALL RIGHT GUYS COMING BACK TO OUR\\nPROJECT RIGHT SO WHAT EXACTLY ARE WE\\nGONNA DO HERE\\nWELL YOU SEE YOU MIGHT HAVE HEARD OF\\nTHESE FILTERS RIGHT LIKE ANYTHING ON\\nSNAPCHAT OR WE ALSO HAVE THIS PIXAR\\nWHEREIN WE PROVIDE ANY INPUT IMAGE AND\\nYOU KNOW WE GET THIS EDGES AND IT'S\\nSOMETHING WE ARE TRYING TO CARTONIFY THE\\nIMAGE\\nSO SIMILAR THING IS WHAT WE ARE GOING TO\\nDO HERE USING PYTHON\\nAND THE LIBRARY THAT I'M GOING TO USE\\nOVER HERE IS GOING TO BE OPENCV ALL\\nRIGHT\\nSO THERE ARE A COUPLE OF STEPS THAT WE\\nARE GOING TO FOLLOW THAT IS GOING TO BE\\nFIRST OFF LOAD OUR IMAGE THEN CREATE AN\\nEDGE MASK\\nTHEN WE GO TO YOU KNOW REDUCE THE NOISE\\nAND ALL SO ON AND SO FORTH\\nSO LET'S NOW MOVE AHEAD AND SEE WHAT\\nWOULD BE ITS UH THE WORKFLOW\\nOKAY FIRST OFF WHAT WE ARE GOING TO DO\\nOVER HERE IS LOAD OUR IMAGE\\nSO THE IMAGE CAN BE ANY IMAGE I WOULD\\nSAY YOU KNOW IT'S BETTER TO TAKE IMAGE\\nOF A HUMAN\\nAND THEN WHAT WE'RE GOING TO DO IS WE'RE\\nGOING TO CREATE A MASK FOLLOWED BY\\nREDUCTION OF NOISE\\nMOVING AHEAD THEN WHAT WE'RE GOING TO DO\\nIS REDUCE THE COLOR PALETTE OVER HERE\\nWHAT DOES THIS COLOR PALETTE MEAN IS\\nLET'S SAY I JUST WANT TO CREATE A\\nCARTOON WHICH HAS ONLY FOUR NUMBER OF\\nCOLORS RIGHT SO THIS IS WHAT WE'RE GOING\\nTO DO\\nAND FINALLY WE ARE GOING TO COMBINE OUR\\nEDGE MASK AND COLOR PALETTE IN ORDER TO\\nCREATE A CARTOON OF ANY GIVEN IMAGE OVER\\nHERE\\nSO WHAT ARE THE DIFFERENT TOOLS AND\\nFRAMEWORKS THAT WE ARE GOING TO USE SO\\nTO START OFF WHAT WE ARE GOING TO DO IS\\nWE ARE GOING TO USE JUPYTER NOTEBOOK\\nALL RIGHT AND THE MAJOR LIBRARY OR THE\\nIMPORTANT LIBRARY THAT WE'RE GOING TO\\nUSE OVER HERE IS GOING TO BE OPENCV 2\\nALL RIGHT AND THEN AS YOU ALL KNOW\\nCOUPLE OF STANDARD LIBRARIES THAT WE'RE\\nGOING TO HAVE IS MATPLOTLIB AND NUMPY\\nAND BEFORE YOU MOVE AHEAD GUYS THE\\nKNOWLEDGE ON PYTHON IS PRETTY ESSENTIAL\\nHERE AND IF YOU'RE SOMEONE WHO IS\\nTOTALLY NEW TO PYTHON PLEASE CHECK OUT\\nTHE PYTHON FULL COURSE LINK\\nWHICH WILL BE GIVEN IN THE DESCRIPTION\\nBOX BELOW SO NOW THAT WE KNOW A BIT\\nABOUT WHAT'S GOING TO HAPPEN AND WHAT WE\\nARE GOING TO WORK ON LET ME QUICKLY JUMP\\nTO MY CODE EDITOR AND SHOW YOU HOW WE\\nCAN BUILD THIS INTERESTING PROJECT ALL\\nRIGHT SO HERE WE ARE THIS IS MY VISUAL\\nSTUDIO CODE\\nALL RIGHT AND NOW WHAT WE'RE GOING TO DO\\nIS FIRST OFF WE'LL TRY TO WRITE DOWN THE\\nSTEPS OKAY LET ME\\nOR BEFORE THAT WHAT WE'LL DO IS LET'S\\nTRY TO IMPORT LIBRARIES FIRST\\nSO WE HAVE IMPORT CV2\\nTHEN WE ALSO NEED IMPORT NUMPY\\nAS NP THEN WE HAVE IMPORT\\nMATPLOTLIB DOT PI PLOT\\nAS PLT ALL RIGHT AND LET'S NOW EXECUTE\\nTHIS OVER HERE\\nFINE NOW IN THE FIRST STAGE OVER HERE\\nLET ME GIVE A COMMENT\\nWE ARE GOING TO LOAD OUR IMAGE AND FOR\\nTHIS WE ARE GOING TO DEFINE A FUNCTION\\nLETS SAY\\nDEF READ FILE AND HERE WE'RE GOING TO\\nGIVE THE PATH\\nSO IT WILL LET'S SAY THIS NAME HAS FILE\\nNAME\\nALL RIGHT AND WE'LL HAVE IMAGE SO THIS\\nWILL BE CV2\\nDOT IMREAD AND YOU JUST HAVE TO GIVE A\\nFILE PATH\\nAND THEN IF YOU WANT TO SEE THE IMAGE\\nIT'S GOING TO BE CV2\\nUNDERSCORE IM SHOW AND WE ARE GOING TO\\nPASS THE FILE NAME\\nOR WHAT WE CAN DO IS IF YOU WANT TO JUST\\nSEE THE IMAGE RIGHT I CAN JUST USE\\nPLT DOT I AM SHOW THEN I JUST HAVE TO\\nPASS THIS IMAGE\\nALL RIGHT AND THEN WE HAVE PLT DOT SHOW\\nAND FINALLY WE SHOULD RETURN THE IMAGE\\nSO THIS WOULD RETURN THE BINARY PART\\nRETURN IMAGE\\nSO THIS IS ALL THERE IS SO LET ME SEE\\nOKAY IT'S NOT GOING TO BE IMAGE IT'S\\nGOING TO BE IMG RIGHT SO IMG\\nBUT BEFORE THAT WE ALSO KNOW THAT\\nWHENEVER THE FILES ARE READ USING CV2\\nTHEY ARE USUALLY READ INTO\\nBGR FORMAT WE'LL CONVERT THAT INTO RGB\\nOKAY SO WE'LL HAVE IMAGE THEN WE HAVE\\nCV2\\nDOTS CONVERT COLOR THAT IS CVT COLOR\\nHERE WE'RE GONNA PASS THE IMAGE THAT I\\nHAVE THAT'S IMAGE\\nTHEN WE HAVE CV2 DOT COLOR\\nOKAY LET ME JUST REDUCE YEAH SEE COLOR\\nWE WANT THIS FOR\\nBGR TO BINARY YEAH IT'S GONNA BE FOR RGB\\nRIGHT SO\\nR G AND B\\nOKAY SO THIS LOOKS GREAT AND I THINK\\nTHIS SHOULD BE FINE\\nLET'S SAVE THIS AND LET'S EXECUTE THIS\\nOVER HERE\\nCOOL SO NOW WHAT I'LL DO IS I'LL ADD AN\\nIMAGE TO OUR DIRECTORY HERE LET ME\\nQUICKLY BROWSE GOOGLE AND ADD AN IMAGE\\nOVER HERE\\nALL RIGHT SO AS YOU CAN SEE HERE I HAVE\\nLOADED MY IMAGE AND\\nGIVE THIS FUNCTION CALL LET'S CALL THIS\\nHERE\\nPASTE IT HERE AND LET'S SAY FILE PATH OR\\nFILE NAME RIGHT\\nSO WE'LL PASTE THIS HERE FILE NAME\\nAND WE'LL PASS A STRING OVER HERE SO\\nWE'LL JUST TAKE THIS AS THIS IS IN THE\\nSAME DIRECTORY WE'LL JUST SAY RELATIVE\\nPATH\\nAND PASS IT OVER HERE AND LET'S TRY TO\\nEXECUTE THIS LET'S SEE IF THIS WORKS\\nSO AS YOU CAN SEE HERE WE ARE GETTING\\nTHIS IMAGE RIGHT BUT\\nNOW I DON'T WANT I DON'T WANT TO SEE\\nWHAT IS THIS ARRAY HERE SO FOR THAT\\nTHE REASON WHY WE ARE GETTING THIS ARRAY\\nIS BECAUSE I'M TRYING TO READ THIS IMAGE\\nHERE\\nSO IF I ADD THIS TO A CERTAIN NAME LET'S\\nSAY\\nIMAGE THEN WE WON'T BE SEEING THAT PART\\nOVER HERE\\nSO AS YOU CAN SEE OVER HERE THIS IS THE\\nIMAGE WHICH WE HAVE LOADED\\nIF YOU WANT TO TURN OFF THE ACCESS WE\\nCAN DO THAT AS WELL PLT DOT\\nACCESS JUST PUT OFF OVER HERE\\nSO IF YOU WANT PLT DOT ACCESS TO BE\\nTURNED OFF YOU HAVE TO USE COUPLE OF\\nLIBRARIES\\nOR I SHOULD SAY A LIBRARY THAT IS VERY\\nSPECIFIC TO JUPYTER NOTEBOOK SO FOR THAT\\nREASON WE'LL JUST AVOID THIS FOR NOW\\nAND JUST FOCUS ON THIS PART\\nALL RIGHT SO AS YOU CAN SEE OUR IMAGE IS\\nLOADING UP PERFECTLY FINE\\nSO NOW WHAT WE ARE GOING TO DO IS WE ARE\\nGOING TO TRY TO CREATE A CARTOON OF THIS\\nAND IF YOU ARE SOMEONE WHO IS WELL\\nVERSED WITH ANIMATION YOU WOULD KNOW\\nTHAT YOU KNOW EDGES PLAY A PRETTY HUGE\\nROLE WHEN IT COMES TO CARTOONS RIGHT\\nSO WHAT WE'LL DO IS WE'LL TRY TO\\nINCREASE THE EDGES OF OUR IMAGE OVER\\nHERE\\nALL RIGHT SO FOR THIS LET'S SAY MARKDOWN\\nAND LET'S GIVE A NAME HERE AS CREATE\\nEDGE MARKS RIGHT SO CREATE\\nEDGE MASK\\nFINE SO WHAT WE'RE GOING TO DO WE HAVE\\nSOMETHING CALLED AS ADAPTIVE THRESHOLD\\nIN OPENCV WHICH WOULD HELP US DO THAT SO\\nWE'LL DEFINE A FUNCTION HERE DEF\\nEDGE MARK ALL RIGHT AND THIS SHOULD TAKE\\nINPUT AS IN IMAGE THEN WE HAVE LINE SIZE\\nTHAT IS THE SIZE THAT YOU WANT TO HAVE\\nFOR YOUR MASK AND BLUR\\nVALUE OKAY SO\\nFIRST OFF WE'LL CONVERT OUR IMAGE FROM\\nRGB TO GRAY\\nWE HAVE CV2 DOT CVT COLOR\\nFIRST OFF IMAGE THE ONE THAT WE ARE\\nGOING TO PASS\\nTHEN WE HAVE CV2 DOT\\nCV OKAY LET ME JUST GO CAPS\\nCOLOR RGB TO GRAY\\nALL RIGHT THIS LOOKS GREAT AND NOW WHAT\\nWE'LL DO IS WE'LL SAY\\nGRAY BLUR SO CV2\\nDOT MEDIAN BLUR WE ARE GOING TO USE\\nTHE OUR GRAY IMAGE ALL RIGHT AND THEN\\nALSO PASS THE BLUR VALUE OKAY\\nAND NOW WHAT WE'LL DO IS WE'LL TAKE THE\\nEDGES HERE\\nSO FOR THIS WE HAVE CV2 NOT ADAPTIVE\\nTHRESHOLD\\nALL RIGHT GUYS SO WITHIN THIS UH THIS\\nTAKES UP COUPLE OF PARAMETERS THE FIRST\\nONE OBVIOUSLY IS GOING TO BE\\nTHE IMAGE SO IT'S GOING TO BE GREY BLUR\\nSO LET ME\\nCOPY THIS FROM HERE MEANWHILE LET ME\\nALSO JUST CHANGE THIS VALUE\\nALL RIGHT AND LET ME COPY THIS FROM HERE\\nAND PASTE IT HERE\\nALL RIGHT AND THEN WE'LL GIVE 255 THAT'S\\nADAPTIVE METHOD AND THEN\\nWE ALSO HAVE THIS BUILT-IN FUNCTION CV2\\nDOT\\nADAPTIVE THRESHOLD MEAN\\nOKAY AND THEN FINALLY WE HAVE\\nBINARY CV2 DOT\\nTHRESHOLD BINARY AND THEN FINALLY WE\\nHAVE LINE SIZE THIS IS THE\\nPARAMETER THAT WE ARE GOING TO GIVE OUT\\nTHAT IS THE THICKNESS OF THE\\nEDGES SO LET'S GIVE THAT AS WELL LINE\\nSIZE\\nAND THEN BLUR VALUE ALL RIGHT\\nSO THIS IS ALL THERE IS OVER HERE AND\\nFINALLY OKAY LET ME JUST DROP THIS DOWN\\nSO THAT IT'S EASIER FOR US TO UNDERSTAND\\nAND FOLLOW UP\\nALL RIGHT AND NOW THIS THING WILL RETURN\\nJUST THE EDGES OKAY\\nEDGES I WILL COPY THE EDGES FROM HERE\\nSO BASICALLY WHAT THIS THING IS DOING IS\\nTHIS IS TAKING IN THE GRAY SCALE LET ME\\nGIVE A COMMENT SO HERE\\nSO THIS TAKES IN GRAYSCALE IMAGE\\nAND OUTPUT OUTPUT HERE IS GOING TO BE\\nJUST THE\\nEDGES RIGHT\\nALL RIGHT SO THIS IS WHAT IS GOING TO\\nHAPPEN HERE LET ME RUN THIS OFF\\nSO AS YOU CAN SEE WE HAVE SUCCESSFULLY\\nRUN THIS AND NOW ALL WE NEED TO DO IS\\nCALL THIS FUNCTION ALL RIGHT WE HAVE TO\\nCALL THIS FUNCTION SO LET ME COPY THIS\\nFROM HERE\\nAND PASTE IT ALL RIGHT AND NOW WE NEED\\nTHE LINE SIZE\\nAND THE BLUR VALUE SO LET'S SAY LINE\\nSIZE WILL GIVE HERE AS\\nFIVE PIXELS YOU CAN GIVE ANY VALUE THAT\\nYOU WANT AND LET ME GIVE FOR BLUR VALUE\\nALSO AS\\nSEVEN SEVEN AND SEVEN EITHER WAY IT\\nDOESN'T MATTER\\nAND THE IMAGE OVER HERE IS GONNA BE THIS\\nONE RIGHT SO OUR INPUT IMAGE\\nALL RIGHT SO HERE I HAVE DONE A SMALL\\nMISTAKE IT'S NOT A GRAYSCALE IMAGE THIS\\nIS GOING TO BE\\nINPUT IMAGE ALL RIGHT WE ARE DONE WITH\\nTHIS\\nLET ME RUN THIS AS WELL SO AS YOU CAN\\nSEE THIS RETURNS AS ONE\\nMATRIX WHAT I'M GOING TO DO IS I LET'S\\nPLOT HOW DOES THIS LOOKS LIKE\\nSO LET'S SAY EDGES IS EQUAL TO THIS AND\\nLET'S TRY TO PLOT THIS OFF\\nPLT DOT IM SHOW EDGES\\nALL RIGHT AND THEN PLT DOT SHOW\\nSO AS YOU CAN SEE THIS IS WHAT WE ARE\\nGETTING HERE IF YOU WANT ME TO ENHANCE\\nTHIS LIKE WHAT I CAN DO IS C\\nMAP IS EQUAL TO GRAY\\nOKAY THAT'S BECAUSE WE GAVE THIS AS AN\\nUPPERCASE LET'S RUN THIS AGAIN\\nSO AS YOU CAN SEE HERE WE HAVE JUST THE\\nEDGES RIGHT\\nIF YOU WANT TO REDUCE THE THICKNESS OF\\nTHIS IT'S PRETTY EASY WE JUST HAVE TO\\nCHANGE THE LINE SIZE LET'S SAY 5 PIXELS\\nAND NOW SEE\\nTHE THICKNESS HAS REDUCED ALL RIGHT AND\\nWE CAN ALSO GIVE C MAP IS EQUAL TO\\nBINARY\\nNOW YOU'LL SEE A BLACK IMAGE WITH A\\nBLACK BACKGROUND WE HAVE JUST WHITE ONES\\nSO EVEN THIS IS FINE EITHER WAY IT\\nDOESN'T MATTER RIGHT BECAUSE\\nTHIS IS THE OUTPUT THAT WE HAVE SHOWING\\nOVER HERE OKAY SO NOW THAT WE HAVE GOT\\nTHE UH IMAGES\\nWHAT WE ARE GOING TO DO IS REDUCE THE\\nCOLOR PALETTE SO WHAT DOES THIS COLOR\\nPALETTE MEAN LET ME SCROLL UP\\nSO AS YOU CAN SEE OVER HERE WE HAVE\\nMULTIPLE COLORS HERE\\nNOW WHAT I WANT TO DO IS I DON'T WANT\\nALL OF THESE COLORS LET'S SAY I WANT TO\\nFOCUS ON JUST\\nYOU KNOW JUST THREE COLORS OR SO SO\\nTHAT'S WHAT WE ARE GOING TO DO HERE\\nLET'S SAY LET ME GIVE A COMMENT HERE\\nREDUCE THE COLOR PALETTE\\nAND LET ME REDUCE THIS AS WELL OKAY\\nSO NOW WHAT'S GOING TO HAPPEN AS I\\nMENTIONED EARLIER WE JUST WANT TO REDUCE\\nTHE NUMBER OF COLORS THAT WE ARE TRYING\\nTO USE HERE\\nSO EVEN FOR THIS WE ARE GOING TO WRITE A\\nFUNCTION THAT IS DEF\\nCOLOR QUANTIZATION AND THIS TAKES IN THE\\nINPUT THAT IS THE IMAGE\\nAND THEN LET'S SAY WE'LL DEFINE A\\nVARIABLE HERE CALLED AS K\\nOR C ANY VARIABLE THAT YOU CAN TAKE AND\\nTHIS BASICALLY REFERS TO YOU KNOW HOW\\nMANY NUMBER OF COLORS YOU WANT TO\\nSHOW SO FIRST OFF WE'LL TRANSFORM THE\\nIMAGE SO\\nFIRST OFF THAT WILL BE TRANSFORM THE\\nIMAGE\\nWHAT THIS MEANS IS UH WE'LL TRY TO\\nRESHAPE THIS OFF\\nSO WE'LL HAVE DATA IS EQUAL TO NP DOT\\nFLOAT\\nAND WE'RE GONNA PASS IMAGE HERE THIS IS\\nGONNA BE THE INPUT IMAGE\\nRIGHT AND THEN WE'LL JUST RESHAPE MINUS\\nONE COMMA THREE\\nTHEN LET'S DETERMINE THE CRITERIA\\nAND HERE WE'LL HAVE LIKE SOMETHING LIKE\\nCV2 DOT\\nTERM CRITERIA\\nALL RIGHT PLUS CV2\\nSORRY THIS IS GOING TO BE LOWERCASE CV2\\nDOT TERM CRITERIA MAX ITERATOR\\nALL RIGHT AND NOW WE'LL GIVE 20 HERE\\nAND THEN 0.01 THIS LOOKS GREAT\\nAND NOW FINALLY WE ARE GOING TO\\nIMPLEMENT HERE K MEANS CLUSTERING RIGHT\\nOKAY SO WE HAVE RET THEN WE HAVE LABEL\\nAND THEN CENTER SO WHAT WE'LL DO IS CV2\\nDOT K MEANS WE'RE GONNA PASS DATA HERE\\nAND THEN WE NEED THE NUMBER OF CENTROIDS\\nTHAT IS K\\nTHEN WE NEED THE LABEL SO THAT IS NONE\\nOVER HERE THEN WE HAVE THE CRITERIA THAT\\nWE WANT\\nLET ME JUST COPY THIS FROM HERE AND ADD\\nIT HERE\\nAND THEN WE'LL SAY 10 AND THEN WE'LL\\nHAVE CB2 DOT\\nLIKE RANDOM K MEANS RIGHT SO C V TWO DOT\\nK MEANS RANDOM CLUSTERS RANDOM CENTERS\\nOKAY SO BASICALLY WE'RE ALSO TRYING TO\\nHAVE A MACHINE LEARNING OVER HERE\\nPART OF IT ALL RIGHT SO NOW WE'LL HAVE\\nCENTERS\\nLET'S SAY CENTER WE HAVE CENTER IS EQUAL\\nTO\\nNP DOT UNIT THAT'S UNIT 8 LET'S SAY YEAH\\nAND THEN WE'LL JUST PASS CENTER HERE ALL\\nRIGHT\\nAND FINALLY A RESULT IS EQUAL TO CENTER\\nWHEREIN I HAVE LABEL DOT FLATTEN\\nAND PASS THIS OFF SO FINALLY RESULT\\nIS EQUAL TO RESULT DOT RESHAPE\\nIMAGE SHAPE AND THIS WOULD RETURN THE\\nRESULT\\nSO THAT MEANS WE DO NOT HAVE ANY\\nSYNTACTICAL ERROR AS I HAVE WRITTEN THIS\\nUH AS I HAVE TRIED TO EXECUTE THIS ALL\\nRIGHT SO NOW WHAT WE'LL DO\\nWE'LL LET'S SAY LET'S TAKE UP AN IMAGE\\nLET'S SAY\\nIMG THIS WILL BE COLOR QUANTIZATION WE\\nI'LL BASICALLY TRY TO CALL THIS FUNCTION\\nAND NOW HERE K RIGHT WE JUST WANT NUMBER\\nOF DOMINANT COLOR LET'S SAY INSTEAD OF\\nMINI WE'LL JUST GIVE TWO OKAY THIS HAS\\nRAN SUCCESSFULLY\\nLET'S TRY TO UH PLOT THIS UP AND LET'S\\nSEE WHAT HAPPENS SO WE'LL\\nHAVE PLT DOT I AM SURE INSTEAD OF THAT\\nLET'S COPY THIS FROM HERE\\nRIGHT SO PLT DOT IM SHOW LET ME COPY IT\\nFROM HERE\\nAND PASTE IT AND HERE WE WON'T BE HAVING\\nBINARY\\nINSTEAD WE'LL JUST HAVE IMAGE\\nSO AS YOU CAN SEE WE HAVE JUST TWO OF\\nTHE MOST PROMINENT COLORS RIGHT\\nNOW LET'S SAY I JUST WANT THREE COLORS\\nAND NOW IF I EXECUTE THIS OKAY IT'S NOT\\nSHOWING ME HERE LET'S SAY\\nI WANT FIVE COLORS OBVIOUSLY THERE ARE\\nDIFFERENT PROMINENT COLORS\\nWHAT HAPPENS LET'S SEE WHAT HAPPENS IF I\\nJUST GIVE ONE RIGHT LET'S LET'S HAVE A\\nLOOK INTO THIS\\nSO AS YOU CAN SEE WE JUST HAVE ONE\\nPROMINENT COLOR HERE\\nALL RIGHT SO REASON WHY WE ARE GETTING\\nTHIS ERROR RIGHT LIKE THE REASON WHY WE\\nARE NOT GETTING\\nTHE CHANGES IN THE COLOR IS BECAUSE OF\\nMY FAULT WHAT I'VE DONE IS WE ARE TRYING\\nTO CHANGE THE COLOR RIGHT AND WE ARE\\nGIVING THE SAME NAME OVER HERE\\nSO WHAT WE'LL DO INSTEAD OF GIVING THE\\nSAME NAME WE'LL SAY\\nIMAGE QUANTIZE ALL RIGHT IMAGE QUANTIZE\\nAND LET'S TRY TO RERUN EVERYTHING FROM\\nTHE START\\nOKAY SO AS YOU CAN SEE NOW WE'LL JUST WE\\nJUST WANT TO SEE WHAT ARE THE PROMINENT\\nTWO COLORS LET'S TRY TO DO THAT\\nOKAY SO AS YOU CAN SEE THESE ARE THE\\nPROMINENT TWO COLORS THAT WE HAVE\\nRIGHT THE GREEN AND THE GRAY ONES ALL\\nRIGHT SO THIS IS ALL THERE IS ABOUT YOU\\nKNOW\\nQUANTIZATION OF A COLOR SO WHAT WE'LL DO\\nNOW IN THE NEXT STAGE\\nWE'RE GOING TO MERGE WHATEVER THE IMAGE\\nTHAT WE HAVE QUANTIZED RIGHT LIKE THE\\nLIMITED NUMBER OF IMAGES\\nAND THEN THESE BOUNDARIES OKAY AND LET'S\\nSEE WHAT HAPPENS\\nTHAT IS LET ME GIVE A COMMENT HERE YOU\\nKNOW COMBINE\\nEDGE MASK WITH\\nTHE QUANTIZED COLOR OKAY THAT IS\\nQUANTIZED IMAGE OVER HERE\\nOKAY BUT BEFORE THAT WHAT WE'LL DO IS\\nLET'S TRY TO ADD UP A FILTER JUST SO\\nTHAT WE REDUCE SOME MORE NOISE SO LET ME\\nGIVE\\nTHAT AS A COMMENT HERE LET'S SAY WE WANT\\nTO REDUCE THE NOISE\\nSO IN ORDER TO REDUCE THE NOISE WE'RE\\nGOING TO PASS THIS THROUGH A FILTER\\nLET'S SAY WE HAVE\\nTHIS BLURRED RIGHT SO BLU R E D LET'S\\nSAY BLURRED THEN WE HAVE THIS FILTER CV2\\nDOT\\nBILATERAL FILTER OKAY BILATERAL FILTER\\nAND NOW WE JUST HAVE TO PASS THE IMAGE\\nTHAT IS THE IMAGE THAT HAS BEEN\\nQUANTIZED\\nTHEN WE HAVE D IS EQUAL TO 7 D HERE IS\\nNOTHING BUT DIAMETER OF EACH PIXEL SO\\nTHAT'S GONNA BE SEVEN AND THEN WE HAVE\\nSIGMA COLOR\\nSO LET'S GIVE THIS AS 200 AND THEN WE\\nHAVE SIGMA SPACE\\nI WILL GIVE EVEN THIS AS 200 OVER HERE\\nAND NOW WHAT WE'LL DO LET'S TRY TO PLOT\\nTHIS AS WELL\\nLET ME GET THIS AND PLOT IT OVER HERE\\nAND WE JUST NEED TO SEE HOW THIS WOULD\\nLOOK LIKE WE'LL COPY THIS\\nAND PASTE IT OVER HERE SO AS YOU CAN SEE\\nOUR IMAGE IS SUBSTANTIALLY BLURRED UP\\nRIGHT SO COMPARED TO WHAT IT WAS OVER\\nHERE\\nALL RIGHT SO IN THE NEXT STAGE AS I\\nMENTIONED EARLIER WE ARE GOING TO\\nCOMBINE\\nTHE EDGE MASK WITH THE QUANTIZES IMAGE\\nRIGHT SO WE ARE GOING TO COMBINE THIS\\nWITH OUR EDGES OVER HERE SO LET'S WRITE\\nA FUNCTION FOR THAT\\nOKAY INSTEAD OF WRITING A FUNCTION WE\\nCAN JUST DIRECTLY PASS THIS THROUGH HERE\\nOR YOU CAN ALSO WRITE\\nTHE FUNCTION SO YOU KNOW LET'S SAY DEF\\nCARTOON\\nALL RIGHT AND THIS THING WHAT IT DO IT\\nWOULD DO IS IT TAKES TWO PARAMETERS\\nOKAY OR LET'S SAY IT WOULD JUST TAKE\\nTHIS BLURRED IMAGE PARAMETER RIGHT SO IT\\nWILL JUST TAKE THIS PARAMETER HERE\\nBLURRED AND LET ME PASTE IT OVER HERE SO\\nFOR THIS WHAT WE ARE GOING TO DO IS\\nLET'S SAY WE HAVE C\\nTHEN WE HAVE CV2 DOT BITWISE AND\\nAND THIS WOULD JUST LIKE END UP THIS\\nVALUE\\nBLURRED LET ME COPY THIS FROM HERE\\nAND THEN WE NEED MASK EDGES RIGHT SO WE\\nCOPY THIS\\nAND WE ALSO HAVE THIS MASK RIGHT SO MASK\\nSO THIS WOULD BE THE EDGES VALUE AND THE\\nEDGES VALUE IS SOMEWHERE OVER HERE\\nON TOP THIS VALUE RIGHT SO THIS IS EDGES\\nSO LET ME COPY THE EDGES HERE\\nAND LET ME PASTE IT HERE ALL RIGHT AND\\nWHAT\\nTHIS FUNCTION WILL DO IS THIS WOULD BE\\nLIKE PLT DOT PLOT\\nSORRY IT'S GOING TO BE I AM SURE WE'RE\\nGOING TO SHOW C\\nAND THEN PLT DOT SHOW\\nWE'LL DO ONE THING LET'S NOT SEND ANY\\nPARAMETER HERE WE'LL JUST\\nLET IT BE THE WAY IT IS AND NOW ALL WE\\nNEED TO DO IS HAVE A FUNCTION CALL\\nLET'S SAY JUST CARTOON AND LET'S SEE\\nWHAT THIS WOULD LOOK LIKE\\nSO AS YOU CAN SEE HERE WE HAVE TRIED TO\\nCREATE A CARTOON IMAGE OF THIS\\nSO LET'S GIVE SOME MORE TRIAL HERE\\nINSTEAD OF THIS\\nLET'S TRY REDUCING THIS TO THREE AND LET\\nME\\nCHANGE THIS FROM K TWO TO NINE AND LET\\nME RE-EXECUTE THIS AND LET'S SEE WHAT\\nHAPPENS\\nOKAY SO AS YOU CAN SEE HERE WE ARE\\nGETTING SOME OF THE DOMINANT OTHER\\nCOLORS\\nAND LET'S NOW SEE HOW THIS CARTOON WOULD\\nLOOK LIKE YES SO AS YOU CAN SEE HERE\\nWE HAVE TRIED TO MAKE CARTOON UNIFIED\\nVERSION OF THIS RIGHT\\nSO LET ME PLOT THIS SIDE BY SIDE LET'S\\nALSO SEE THE ORIGINAL IMAGE\\nFOR THAT WHAT I'LL DO IS I'LL DO A SMALL\\nCHANGE THAT IS\\nALL RIGHT WE'LL TRY TO RUN EVERYTHING\\nHERE\\nAND FINALLY LET'S TRY TO PLOT THIS UP\\nOKAY SO WE'LL ALSO TRY TO PLOT OUR\\nORIGINAL IMAGE SO LET ME COPY THIS\\nAND WRITE ONE MORE FUNCTION OKAY AND\\nLET'S EXECUTE THIS\\nSO AS YOU CAN SEE HERE THIS IS OUR\\nORIGINAL IMAGE LET ME ZOOM OUT\\nUH BEFORE THAT LET ME ALSO GIVE THIS\\nTHING HERE TITLE PLT DOT TITLE\\nORIGINAL IMAGE AND THIS IS GONNA BE\\nCARTOONIFIED IMAGE RIGHT SO\\nAND LET'S TRY TO RE RUN THIS AND LET'S\\nSEE WHAT HAPPENS\\nSO AS YOU CAN SEE HERE WE HAVE ORIGINAL\\nIMAGE AND THE CARD UNIFIED IMAGE\\nLET'S TRY DOING THIS WITH SOME OTHER\\nIMAGES\\nLET ME TAKE COUPLE MORE IMAGES FROM\\nGOOGLE AND LET'S SEE WHAT HAPPENS\\nALRIGHT GUYS SO LET'S TRY TO LOAD\\nANOTHER IMAGE LET ME JUST COME HERE\\nWE HAVE THIS ANOTHER IMAGE HERE LET ME\\nCOPY THE RELATIVE PATH\\nAND LET ME ADD THIS PATH OVER HERE ALL\\nRIGHT AND LET ME RUN\\nALL AND LET'S SEE WHAT HAPPENS SO THIS\\nIS OUR ORIGINAL IMAGE\\nLET'S SEE THE ORIGINAL AND EVERYTHING AT\\nTHE END\\nSO LET'S SEE WHAT HAPPENS\\nSO AS YOU CAN SEE HERE WE HAVE TRIED TO\\nBLURRED AND WE ARE TRYING TO CARTOONY\\nFIVE\\nYOU KNOW THE IMAGES THAT WE HAVE WELL\\nYOU CAN DO SOME MORE MODIFICATIONS YOU\\nCAN INCREASE THE THICKNESS OF YOUR LINE\\nAND UH THAT WOULD EVEN MORE ENHANCE THE\\nLOOK AND FEEL OF YOUR\\nOF YOUR IMAGE SO LET'S SAY I WANT TO\\nCHANGE THIS FROM FIVE LET'S SAY EIGHT\\nAND EVEN THIS LET'S SAY EIGHT AND TRY TO\\nRUN ALL OF THIS\\nFROM THE START\\nOKAY THE REASON WHY WE ARE GETTING THIS\\nERROR IS BECAUSE WE HAVE TO RUN THIS\\nFROM THE START RIGHT SO LET ME\\nJUST RESTART THIS KERNEL AND RUN IT ALL\\nAGAIN OKAY\\nAND THIS VALUE WOULD RANGE FROM SEVEN\\nCOMMA SEVEN ALL RIGHT AND LET'S TRY TO\\nJUST RERUN THIS AND RUN ALL\\nOKAY SO AS YOU CAN SEE HERE WE ARE\\nGETTING THIS IMAGE\\nUH THIS IS THE ORIGINAL IMAGE THE WAY IT\\nIS AND THEN WE TRY TO CARTONIFY IT AND\\nTHIS IS HOW IT WOULD LOOK LIKE\\nWELL YOU CAN MODIFY THIS IN YOUR OWN WAY\\nAND THIS IS JUST THE TIP OF ICEBERG THAT\\nYOU CAN DO WITH OPENCV\\nAND YOU KNOW IF YOU HAVE ANY UH IF YOU\\nHAVE PERFORMED ANY IMPROVEMENTS YOU KNOW\\nYOU CAN DEFINITELY\\nPROVIDE YOUR GITHUB LINK IN THE COMMENT\\nBOX BELOW ALL RIGHT GUYS WITH THIS WE\\nCOME TO THE END OF OUR SESSION I HOPE\\nYOU ENJOYED AND LEARNED SOMETHING NEW\\nIF YOU HAVE ANY FURTHER QUERIES PLEASE\\nDO MENTION THEM IN THE COMMENT BOX BELOW\\nUNTIL NEXT TIME GOODBYE AND TAKE CARE I\\nHOPE YOU HAVE ENJOYED LISTENING TO THIS\\nVIDEO\\nPLEASE BE KIND ENOUGH TO LIKE IT AND YOU\\nCAN COMMENT\\nANY OF YOUR DOUBTS AND QUERIES AND WE\\nWILL REPLY THEM\\nAT THE EARLIEST DO LOOK OUT FOR MORE\\nVIDEOS IN OUR PLAYLIST\\nAND SUBSCRIBE TO EDUREKA CHANNEL TO\\nLEARN MORE\\nHAPPY LEARNING\\nYOU\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33yR3nwEg0Ak",
        "outputId": "9aa31ccb-b05b-47eb-b837-6eee8d6183ce"
      },
      "source": [
        "with processor.as_target_processor():\n",
        "  labels = processor(target_transcription, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# compute loss by passing labels\n",
        "loss = model(input_values, labels=labels).loss\n",
        "loss.backward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}